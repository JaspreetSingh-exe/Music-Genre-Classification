{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, Bidirectional\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the YAMNET Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_yamnet_features(audio_path):\n",
    "    try:\n",
    "        audio, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
    "        waveform = audio.astype(np.float32)\n",
    "        scores, embeddings, _ = yamnet_model(waveform)\n",
    "        return np.mean(embeddings.numpy(), axis=0)  \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_all_yamnet_features(data_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for genre in os.listdir(data_path):\n",
    "        genre_path = os.path.join(data_path, genre)\n",
    "        if os.path.isdir(genre_path):\n",
    "            for file in os.listdir(genre_path):\n",
    "                if file.endswith(\".wav\"):\n",
    "                    file_path = os.path.join(genre_path, file)\n",
    "                    features = extract_yamnet_features(file_path)\n",
    "                    if features is not None:\n",
    "                        data.append(features)\n",
    "                        labels.append(genre)\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(X_train, y_train):\n",
    "    model = SVC(kernel='rbf', C=10, gamma='scale')\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv1D(128, kernel_size=5, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        Conv1D(256, kernel_size=5, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(128, return_sequences=True, input_shape=input_shape)),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(LSTM(64)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformer(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Dense(128, activation=\"relu\")(inputs)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    transformer_layer = layers.MultiHeadAttention(num_heads=8, key_dim=64)(x, x)\n",
    "    x = layers.Add()([x, transformer_layer])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_audio(audio_path, model, scaler, label_encoder):\n",
    "    features = extract_yamnet_features(audio_path)\n",
    "    if features is not None:\n",
    "        features = scaler.transform([features])\n",
    "        prediction = model.predict(features)\n",
    "        return label_encoder.inverse_transform(prediction)[0]\n",
    "    return \"Error processing audio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load extracted features\n",
    "df = pd.read_csv(\"/Users/js/Desktop/Music Genre Classification/Data/yamnet_features.csv\")\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Genre\"] = label_encoder.fit_transform(df[\"Genre\"])\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "svm_model = train_svm(X_train, y_train)\n",
    "cnn_model = create_cnn((X_train.shape[1], 1), len(label_encoder.classes_))\n",
    "lstm_model = create_lstm((X_train.shape[1], 1), len(label_encoder.classes_))\n",
    "transformer_model = create_transformer((X_train.shape[1], 1), len(label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for CNN & LSTM\n",
    "X_train_r = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_r = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 22:32:28.059812: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - ETA: 0s - loss: 48.0003 - accuracy: 0.6097"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 22:32:30.078146: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 3s 68ms/step - loss: 48.0003 - accuracy: 0.6097 - val_loss: 6.6869 - val_accuracy: 0.4611\n",
      "Epoch 2/20\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 35.4516 - accuracy: 0.7444 - val_loss: 12.5825 - val_accuracy: 0.3833\n",
      "Epoch 3/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 36.4584 - accuracy: 0.7861 - val_loss: 32.8617 - val_accuracy: 0.2500\n",
      "Epoch 4/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 30.1364 - accuracy: 0.8125 - val_loss: 35.3308 - val_accuracy: 0.3222\n",
      "Epoch 5/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 28.9271 - accuracy: 0.8583 - val_loss: 34.1581 - val_accuracy: 0.3667\n",
      "Epoch 6/20\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 22.8079 - accuracy: 0.8583 - val_loss: 37.3608 - val_accuracy: 0.3333\n",
      "Epoch 7/20\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 22.2125 - accuracy: 0.8514 - val_loss: 35.0457 - val_accuracy: 0.3778\n",
      "Epoch 8/20\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 14.3030 - accuracy: 0.8903 - val_loss: 46.7942 - val_accuracy: 0.4000\n",
      "Epoch 9/20\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 25.2019 - accuracy: 0.8792 - val_loss: 46.8499 - val_accuracy: 0.4222\n",
      "Epoch 10/20\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 22.3554 - accuracy: 0.8694 - val_loss: 30.8548 - val_accuracy: 0.5722\n",
      "Epoch 11/20\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 20.0626 - accuracy: 0.8875 - val_loss: 23.6767 - val_accuracy: 0.6778\n",
      "Epoch 12/20\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 16.9858 - accuracy: 0.9042 - val_loss: 27.6483 - val_accuracy: 0.6778\n",
      "Epoch 13/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 17.4966 - accuracy: 0.9042 - val_loss: 26.2729 - val_accuracy: 0.7056\n",
      "Epoch 14/20\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 14.7739 - accuracy: 0.8944 - val_loss: 20.6282 - val_accuracy: 0.7722\n",
      "Epoch 15/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 20.8949 - accuracy: 0.8917 - val_loss: 20.1661 - val_accuracy: 0.7722\n",
      "Epoch 16/20\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 11.0770 - accuracy: 0.9292 - val_loss: 26.5737 - val_accuracy: 0.7167\n",
      "Epoch 17/20\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 9.7731 - accuracy: 0.9389 - val_loss: 28.9225 - val_accuracy: 0.7444\n",
      "Epoch 18/20\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 9.3063 - accuracy: 0.9458 - val_loss: 23.6738 - val_accuracy: 0.8000\n",
      "Epoch 19/20\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 11.0227 - accuracy: 0.9278 - val_loss: 26.9682 - val_accuracy: 0.8167\n",
      "Epoch 20/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 8.8210 - accuracy: 0.9347 - val_loss: 26.9043 - val_accuracy: 0.8278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x414c33e20>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(X_train_r, y_train, epochs=20, batch_size=32, validation_data=(X_test_r, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 22:33:35.236536: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-13 22:33:35.657348: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-13 22:33:35.676757: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-13 22:33:35.890722: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-13 22:33:35.910128: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-13 22:33:36.094725: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-13 22:33:36.121579: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-13 22:33:36.410318: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-13 22:33:36.446722: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - ETA: 0s - loss: 1.9967 - accuracy: 0.3111"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 22:33:45.182215: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-13 22:33:45.345101: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-13 22:33:45.358779: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-13 22:33:45.517505: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-13 22:33:45.535642: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 14s 430ms/step - loss: 1.9967 - accuracy: 0.3111 - val_loss: 1.6473 - val_accuracy: 0.3778\n",
      "Epoch 2/20\n",
      "23/23 [==============================] - 8s 342ms/step - loss: 1.6015 - accuracy: 0.4194 - val_loss: 1.4402 - val_accuracy: 0.5167\n",
      "Epoch 3/20\n",
      "23/23 [==============================] - 7s 322ms/step - loss: 1.4235 - accuracy: 0.4847 - val_loss: 1.2439 - val_accuracy: 0.6167\n",
      "Epoch 4/20\n",
      "23/23 [==============================] - 7s 324ms/step - loss: 1.2765 - accuracy: 0.5583 - val_loss: 1.2316 - val_accuracy: 0.6000\n",
      "Epoch 5/20\n",
      "23/23 [==============================] - 8s 338ms/step - loss: 1.1947 - accuracy: 0.5917 - val_loss: 1.1209 - val_accuracy: 0.6444\n",
      "Epoch 6/20\n",
      "23/23 [==============================] - 8s 336ms/step - loss: 1.0888 - accuracy: 0.6444 - val_loss: 1.0366 - val_accuracy: 0.6778\n",
      "Epoch 7/20\n",
      "23/23 [==============================] - 8s 342ms/step - loss: 1.0902 - accuracy: 0.6375 - val_loss: 1.0766 - val_accuracy: 0.6611\n",
      "Epoch 8/20\n",
      "23/23 [==============================] - 8s 337ms/step - loss: 1.0689 - accuracy: 0.6458 - val_loss: 0.9805 - val_accuracy: 0.6556\n",
      "Epoch 9/20\n",
      "23/23 [==============================] - 8s 334ms/step - loss: 0.9470 - accuracy: 0.6833 - val_loss: 0.8916 - val_accuracy: 0.7056\n",
      "Epoch 10/20\n",
      "23/23 [==============================] - 8s 331ms/step - loss: 0.9034 - accuracy: 0.7111 - val_loss: 0.9569 - val_accuracy: 0.7000\n",
      "Epoch 11/20\n",
      "23/23 [==============================] - 8s 333ms/step - loss: 0.8905 - accuracy: 0.6944 - val_loss: 0.9048 - val_accuracy: 0.7000\n",
      "Epoch 12/20\n",
      "23/23 [==============================] - 8s 334ms/step - loss: 0.8832 - accuracy: 0.7028 - val_loss: 0.8919 - val_accuracy: 0.7167\n",
      "Epoch 13/20\n",
      "23/23 [==============================] - 8s 347ms/step - loss: 0.8232 - accuracy: 0.7403 - val_loss: 0.8487 - val_accuracy: 0.7111\n",
      "Epoch 14/20\n",
      "23/23 [==============================] - 8s 327ms/step - loss: 0.7804 - accuracy: 0.7542 - val_loss: 0.8955 - val_accuracy: 0.7333\n",
      "Epoch 15/20\n",
      "23/23 [==============================] - 8s 328ms/step - loss: 0.7683 - accuracy: 0.7472 - val_loss: 0.8905 - val_accuracy: 0.7056\n",
      "Epoch 16/20\n",
      "23/23 [==============================] - 7s 324ms/step - loss: 0.7536 - accuracy: 0.7472 - val_loss: 0.8282 - val_accuracy: 0.7111\n",
      "Epoch 17/20\n",
      "23/23 [==============================] - 7s 323ms/step - loss: 0.7302 - accuracy: 0.7597 - val_loss: 0.8367 - val_accuracy: 0.7333\n",
      "Epoch 18/20\n",
      "23/23 [==============================] - 7s 324ms/step - loss: 0.7213 - accuracy: 0.7639 - val_loss: 0.8514 - val_accuracy: 0.7278\n",
      "Epoch 19/20\n",
      "23/23 [==============================] - 7s 324ms/step - loss: 0.6718 - accuracy: 0.7708 - val_loss: 0.8506 - val_accuracy: 0.7556\n",
      "Epoch 20/20\n",
      "23/23 [==============================] - 7s 323ms/step - loss: 0.6811 - accuracy: 0.7819 - val_loss: 0.8163 - val_accuracy: 0.7556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3e57a9df0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(X_train_r, y_train, epochs=20, batch_size=32, validation_data=(X_test_r, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 22:36:46.871583: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-13 22:36:48.983 python[97787:6171080] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-97787-2025-02-13_22_36_48-4089147050‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-02-13 22:36:49.495 python[97787:6171077] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-97787-2025-02-13_22_36_49-2862996541‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-02-13 22:36:49.502 python[97787:6171077] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-97787-2025-02-13_22_36_49-991255949‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-02-13 22:36:49.513 python[97787:6171077] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-97787-2025-02-13_22_36_49-2801799730‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-02-13 22:36:49.520 python[97787:6171077] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-97787-2025-02-13_22_36_49-1615038064‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-02-13 22:36:49.532 python[97787:6171077] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-97787-2025-02-13_22_36_49-1375691838‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - ETA: 0s - loss: 2.7191 - accuracy: 0.1347"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 22:37:45.884380: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 66s 3s/step - loss: 2.7191 - accuracy: 0.1347 - val_loss: 2.0339 - val_accuracy: 0.2111\n",
      "Epoch 2/20\n",
      "23/23 [==============================] - 44s 2s/step - loss: 2.3572 - accuracy: 0.1750 - val_loss: 2.0477 - val_accuracy: 0.1611\n",
      "Epoch 3/20\n",
      "23/23 [==============================] - 42s 2s/step - loss: 2.2507 - accuracy: 0.1944 - val_loss: 2.0431 - val_accuracy: 0.1778\n",
      "Epoch 4/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 2.2852 - accuracy: 0.1917 - val_loss: 2.0258 - val_accuracy: 0.2056\n",
      "Epoch 5/20\n",
      "23/23 [==============================] - 40s 2s/step - loss: 2.3090 - accuracy: 0.1819 - val_loss: 1.9461 - val_accuracy: 0.2056\n",
      "Epoch 6/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 2.1742 - accuracy: 0.2097 - val_loss: 1.9225 - val_accuracy: 0.2111\n",
      "Epoch 7/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 2.1708 - accuracy: 0.2194 - val_loss: 1.9609 - val_accuracy: 0.2278\n",
      "Epoch 8/20\n",
      "23/23 [==============================] - 42s 2s/step - loss: 2.1820 - accuracy: 0.2236 - val_loss: 1.9181 - val_accuracy: 0.2722\n",
      "Epoch 9/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 2.2118 - accuracy: 0.2028 - val_loss: 2.0899 - val_accuracy: 0.2444\n",
      "Epoch 10/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 2.1541 - accuracy: 0.2278 - val_loss: 1.9315 - val_accuracy: 0.2444\n",
      "Epoch 11/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 2.1524 - accuracy: 0.2250 - val_loss: 1.9538 - val_accuracy: 0.2111\n",
      "Epoch 12/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 2.0561 - accuracy: 0.2458 - val_loss: 1.9882 - val_accuracy: 0.2056\n",
      "Epoch 13/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 2.0073 - accuracy: 0.2431 - val_loss: 1.8326 - val_accuracy: 0.3611\n",
      "Epoch 14/20\n",
      "23/23 [==============================] - 40s 2s/step - loss: 1.9718 - accuracy: 0.2764 - val_loss: 1.8342 - val_accuracy: 0.3111\n",
      "Epoch 15/20\n",
      "23/23 [==============================] - 39s 2s/step - loss: 1.9383 - accuracy: 0.2806 - val_loss: 1.8946 - val_accuracy: 0.3000\n",
      "Epoch 16/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 1.9357 - accuracy: 0.2764 - val_loss: 1.7733 - val_accuracy: 0.3389\n",
      "Epoch 17/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 1.8829 - accuracy: 0.3125 - val_loss: 1.8556 - val_accuracy: 0.2722\n",
      "Epoch 18/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 1.8605 - accuracy: 0.3181 - val_loss: 1.7708 - val_accuracy: 0.3111\n",
      "Epoch 19/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 1.8347 - accuracy: 0.3250 - val_loss: 1.6693 - val_accuracy: 0.3444\n",
      "Epoch 20/20\n",
      "23/23 [==============================] - 43s 2s/step - loss: 1.8915 - accuracy: 0.2972 - val_loss: 1.7607 - val_accuracy: 0.3611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3e55b8850>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model.fit(X_train_r, y_train, epochs=20, batch_size=32, validation_data=(X_test_r, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "import joblib\n",
    "svm_model_path = \"/Users/js/Desktop/Music Genre Classification/Models/svm_model.pkl\"\n",
    "cnn_model_path = \"/Users/js/Desktop/Music Genre Classification/Models/cnn_model.h5\"\n",
    "lstm_model_path = \"/Users/js/Desktop/Music Genre Classification/Models/lstm_model.h5\"\n",
    "transformer_model_path = \"/Users/js/Desktop/Music Genre Classification/Models/transformer_model.h5\"\n",
    "\n",
    "\n",
    "joblib.dump(svm_model, svm_model_path)\n",
    "cnn_model.save(cnn_model_path)\n",
    "lstm_model.save(lstm_model_path)\n",
    "transformer_model.save(transformer_model_path)\n",
    "\n",
    "print(\"Music Genre Classification Models Trained and Saved Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAMNet-based classification system is ready!\n"
     ]
    }
   ],
   "source": [
    "# Load YAMNet model\n",
    "yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')\n",
    "# Encode genres\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Genre\"] = label_encoder.fit_transform(df[\"Genre\"])\n",
    "\n",
    "# Split dataset\n",
    "X = df.iloc[:, 1:].values\n",
    "y = df.iloc[:, 0].values\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def extract_yamnet_features(audio_path):\n",
    "    try:\n",
    "        # Load audio file and ensure it is mono\n",
    "        audio, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
    "        \n",
    "        # Ensure waveform is a 1D float32 array\n",
    "        waveform = audio.astype(np.float32)\n",
    "\n",
    "        # Pass the waveform to YAMNet (now correctly shaped)\n",
    "        scores, embeddings, _ = yamnet_model(waveform)\n",
    "\n",
    "        # Extract and return mean embedding\n",
    "        return np.mean(embeddings.numpy(), axis=0)  \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# SVM Model\n",
    "def train_svm(X_train, y_train):\n",
    "    model = SVC(kernel='rbf', C=10, gamma='scale')\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Train SVM on YAMNet embeddings\n",
    "svm_model = train_svm(X_train, y_train)\n",
    "\n",
    "# Function to classify a new audio file\n",
    "def classify_audio(audio_path, model, label_encoder):\n",
    "    features = extract_yamnet_features(audio_path)\n",
    "    if features is not None:\n",
    "        features = scaler.transform([features])\n",
    "        prediction = model.predict(features)\n",
    "        return label_encoder.inverse_transform(prediction)[0]\n",
    "    return \"Error processing audio\"\n",
    "\n",
    "print(\"YAMNet-based classification system is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAMNet feature extraction complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to genres dataset\n",
    "data_path = \"/Users/js/Desktop/Music Genre Classification/Data/genres_original\"\n",
    "\n",
    "# Function to extract YAMNet features for all files\n",
    "def extract_all_yamnet_features(data_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for genre in os.listdir(data_path):\n",
    "        genre_path = os.path.join(data_path, genre)\n",
    "        if os.path.isdir(genre_path):\n",
    "            for file in os.listdir(genre_path):\n",
    "                if file.endswith(\".wav\"):\n",
    "                    file_path = os.path.join(genre_path, file)\n",
    "                    features = extract_yamnet_features(file_path)\n",
    "                    if features is not None:\n",
    "                        data.append(features)\n",
    "                        labels.append(genre)\n",
    "    \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Extract features and labels\n",
    "X_yamnet, y_yamnet = extract_all_yamnet_features(data_path)\n",
    "\n",
    "# Save as CSV for future use\n",
    "df_yamnet = pd.DataFrame(X_yamnet)\n",
    "df_yamnet[\"Genre\"] = y_yamnet\n",
    "df_yamnet.to_csv(\"/Users/js/Desktop/Music Genre Classification/Data/yamnet_features.csv\", index=False)\n",
    "\n",
    "print(\"YAMNet feature extraction complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM trained on YAMNet features!\n"
     ]
    }
   ],
   "source": [
    "# Load YAMNet features dataset\n",
    "df = pd.read_csv(\"/Users/js/Desktop/Music Genre Classification/Data/yamnet_features.csv\")\n",
    "\n",
    "# Encode genre labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Genre\"] = label_encoder.fit_transform(df[\"Genre\"])\n",
    "\n",
    "# Split features and labels\n",
    "X = df.iloc[:, :-1].values  # 1024 YAMNet features\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train new SVM model\n",
    "svm_model = SVC(kernel='rbf', C=10, gamma='scale')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"SVM trained on YAMNet features!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_audio(audio_path, model, scaler, label_encoder):\n",
    "    features = extract_yamnet_features(audio_path)\n",
    "    if features is not None:\n",
    "        features = scaler.transform([features])  # Normalize using the new scaler\n",
    "        prediction = model.predict(features)\n",
    "        return label_encoder.inverse_transform(prediction)[0]\n",
    "    return \"Error processing audio\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hiphop'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_audio('/Users/js/Downloads/action-urban-trap-141691.wav', svm_model, scaler, label_encoder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
