{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, Bidirectional\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the YAMNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reference - https://www.tensorflow.org/hub/tutorials/yamnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 01:56:18.942486: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-02-15 01:56:18.942515: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-02-15 01:56:18.942519: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-02-15 01:56:18.942583: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-02-15 01:56:18.942718: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_yamnet_features(audio_path):\n",
    "    try:\n",
    "        audio, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
    "        waveform = audio.astype(np.float32)\n",
    "        scores, embeddings, _ = yamnet_model(waveform)\n",
    "        return np.mean(embeddings.numpy(), axis=0)  \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_all_yamnet_features(data_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for genre in os.listdir(data_path):\n",
    "        genre_path = os.path.join(data_path, genre)\n",
    "        if os.path.isdir(genre_path):\n",
    "            for file in os.listdir(genre_path):\n",
    "                if file.endswith(\".wav\"):\n",
    "                    file_path = os.path.join(genre_path, file)\n",
    "                    features = extract_yamnet_features(file_path)\n",
    "                    if features is not None:\n",
    "                        data.append(features)\n",
    "                        labels.append(genre)\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(X_train, y_train):\n",
    "    model = SVC(kernel='rbf', C=10, gamma='scale')\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv1D(128, kernel_size=5, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        Conv1D(256, kernel_size=5, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(128, return_sequences=True, input_shape=input_shape)),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(LSTM(64)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformer(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Dense(128, activation=\"relu\")(inputs)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    transformer_layer = layers.MultiHeadAttention(num_heads=8, key_dim=64)(x, x)\n",
    "    x = layers.Add()([x, transformer_layer])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_audio(audio_path, model, scaler, label_encoder):\n",
    "    features = extract_yamnet_features(audio_path)\n",
    "    if features is not None:\n",
    "        features = scaler.transform([features])\n",
    "        prediction = model.predict(features)\n",
    "        return label_encoder.inverse_transform(prediction)[0]\n",
    "    return \"Error processing audio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load extracted features\n",
    "df = pd.read_csv(\"/Users/js/Desktop/Music Genre Classification/Data/yamnet_features.csv\")\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Genre\"] = label_encoder.fit_transform(df[\"Genre\"])\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "svm_model = train_svm(X_train, y_train)\n",
    "cnn_model = create_cnn((X_train.shape[1], 1), len(label_encoder.classes_))\n",
    "lstm_model = create_lstm((X_train.shape[1], 1), len(label_encoder.classes_))\n",
    "transformer_model = create_transformer((X_train.shape[1], 1), len(label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for CNN & LSTM\n",
    "X_train_r = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_r = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 01:56:46.621026: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - ETA: 0s - loss: 19.8050 - accuracy: 0.6597"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 01:56:48.210043: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 2s 53ms/step - loss: 19.8050 - accuracy: 0.6597 - val_loss: 2.7682 - val_accuracy: 0.5944\n",
      "Epoch 2/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 19.5085 - accuracy: 0.7542 - val_loss: 8.8721 - val_accuracy: 0.3778\n",
      "Epoch 3/20\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 14.5577 - accuracy: 0.7917 - val_loss: 33.6012 - val_accuracy: 0.1667\n",
      "Epoch 4/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 12.5419 - accuracy: 0.8181 - val_loss: 16.4284 - val_accuracy: 0.3389\n",
      "Epoch 5/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 11.6872 - accuracy: 0.8417 - val_loss: 26.2646 - val_accuracy: 0.3556\n",
      "Epoch 6/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 8.8866 - accuracy: 0.8639 - val_loss: 17.7561 - val_accuracy: 0.4278\n",
      "Epoch 7/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 9.9172 - accuracy: 0.8708 - val_loss: 35.6903 - val_accuracy: 0.2222\n",
      "Epoch 8/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 7.2850 - accuracy: 0.8764 - val_loss: 29.6629 - val_accuracy: 0.3889\n",
      "Epoch 9/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 8.0463 - accuracy: 0.8708 - val_loss: 17.0031 - val_accuracy: 0.5278\n",
      "Epoch 10/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 6.4767 - accuracy: 0.8903 - val_loss: 24.3152 - val_accuracy: 0.4722\n",
      "Epoch 11/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 6.5366 - accuracy: 0.8986 - val_loss: 17.0299 - val_accuracy: 0.5444\n",
      "Epoch 12/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 6.6935 - accuracy: 0.8847 - val_loss: 9.9627 - val_accuracy: 0.6556\n",
      "Epoch 13/20\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 5.5167 - accuracy: 0.9083 - val_loss: 8.6047 - val_accuracy: 0.7278\n",
      "Epoch 14/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 6.6880 - accuracy: 0.9153 - val_loss: 10.8838 - val_accuracy: 0.7111\n",
      "Epoch 15/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 3.7624 - accuracy: 0.9361 - val_loss: 13.5030 - val_accuracy: 0.7056\n",
      "Epoch 16/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 3.9614 - accuracy: 0.9333 - val_loss: 12.1127 - val_accuracy: 0.7500\n",
      "Epoch 17/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 4.5310 - accuracy: 0.9292 - val_loss: 10.7424 - val_accuracy: 0.7833\n",
      "Epoch 18/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 3.1534 - accuracy: 0.9472 - val_loss: 9.1436 - val_accuracy: 0.8167\n",
      "Epoch 19/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 4.0267 - accuracy: 0.9389 - val_loss: 10.9167 - val_accuracy: 0.8056\n",
      "Epoch 20/20\n",
      "23/23 [==============================] - 1s 31ms/step - loss: 4.1291 - accuracy: 0.9347 - val_loss: 12.2481 - val_accuracy: 0.8056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x32b68c820>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(X_train_r, y_train, epochs=20, batch_size=32, validation_data=(X_test_r, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 01:57:20.016141: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-15 01:57:20.434724: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-15 01:57:20.454501: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-15 01:57:20.641078: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-15 01:57:20.660408: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-15 01:57:20.850452: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-15 01:57:20.877620: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/23 [>.............................] - ETA: 1:30 - loss: 2.2010 - accuracy: 0.0625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 01:57:21.160291: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-15 01:57:21.192272: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - ETA: 0s - loss: 2.0004 - accuracy: 0.2667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 01:57:28.371955: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-15 01:57:28.519392: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-15 01:57:28.532365: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-15 01:57:28.646221: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-15 01:57:28.660806: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 12s 361ms/step - loss: 2.0004 - accuracy: 0.2667 - val_loss: 1.6996 - val_accuracy: 0.3778\n",
      "Epoch 2/20\n",
      "23/23 [==============================] - 8s 328ms/step - loss: 1.6701 - accuracy: 0.4153 - val_loss: 1.4446 - val_accuracy: 0.4944\n",
      "Epoch 3/20\n",
      "23/23 [==============================] - 7s 325ms/step - loss: 1.4563 - accuracy: 0.4750 - val_loss: 1.2779 - val_accuracy: 0.5944\n",
      "Epoch 4/20\n",
      "23/23 [==============================] - 7s 325ms/step - loss: 1.3220 - accuracy: 0.5500 - val_loss: 1.2691 - val_accuracy: 0.5722\n",
      "Epoch 5/20\n",
      "23/23 [==============================] - 7s 323ms/step - loss: 1.2411 - accuracy: 0.5861 - val_loss: 1.1112 - val_accuracy: 0.6778\n",
      "Epoch 6/20\n",
      "23/23 [==============================] - 7s 323ms/step - loss: 1.1561 - accuracy: 0.6056 - val_loss: 1.0838 - val_accuracy: 0.6333\n",
      "Epoch 7/20\n",
      "23/23 [==============================] - 7s 319ms/step - loss: 1.0730 - accuracy: 0.6361 - val_loss: 1.0273 - val_accuracy: 0.6444\n",
      "Epoch 8/20\n",
      "23/23 [==============================] - 7s 326ms/step - loss: 1.0130 - accuracy: 0.6806 - val_loss: 1.0157 - val_accuracy: 0.6778\n",
      "Epoch 9/20\n",
      "23/23 [==============================] - 7s 326ms/step - loss: 0.9384 - accuracy: 0.7042 - val_loss: 0.9109 - val_accuracy: 0.7222\n",
      "Epoch 10/20\n",
      "23/23 [==============================] - 7s 320ms/step - loss: 0.8902 - accuracy: 0.7181 - val_loss: 0.8851 - val_accuracy: 0.7389\n",
      "Epoch 11/20\n",
      "23/23 [==============================] - 7s 320ms/step - loss: 0.8877 - accuracy: 0.7097 - val_loss: 0.8862 - val_accuracy: 0.7111\n",
      "Epoch 12/20\n",
      "23/23 [==============================] - 7s 327ms/step - loss: 0.8128 - accuracy: 0.7444 - val_loss: 0.9190 - val_accuracy: 0.7222\n",
      "Epoch 13/20\n",
      "23/23 [==============================] - 7s 325ms/step - loss: 0.8751 - accuracy: 0.7125 - val_loss: 0.9192 - val_accuracy: 0.7111\n",
      "Epoch 14/20\n",
      "23/23 [==============================] - 7s 324ms/step - loss: 0.8254 - accuracy: 0.7236 - val_loss: 0.8769 - val_accuracy: 0.7389\n",
      "Epoch 15/20\n",
      "23/23 [==============================] - 8s 328ms/step - loss: 0.8087 - accuracy: 0.7500 - val_loss: 0.8860 - val_accuracy: 0.6944\n",
      "Epoch 16/20\n",
      "23/23 [==============================] - 7s 322ms/step - loss: 0.8034 - accuracy: 0.7347 - val_loss: 0.8449 - val_accuracy: 0.7333\n",
      "Epoch 17/20\n",
      "23/23 [==============================] - 7s 317ms/step - loss: 0.7321 - accuracy: 0.7611 - val_loss: 0.8931 - val_accuracy: 0.7222\n",
      "Epoch 18/20\n",
      "23/23 [==============================] - 7s 321ms/step - loss: 0.7900 - accuracy: 0.7458 - val_loss: 0.8677 - val_accuracy: 0.7278\n",
      "Epoch 19/20\n",
      "23/23 [==============================] - 7s 319ms/step - loss: 0.7432 - accuracy: 0.7569 - val_loss: 0.8890 - val_accuracy: 0.7167\n",
      "Epoch 20/20\n",
      "23/23 [==============================] - 7s 323ms/step - loss: 0.7035 - accuracy: 0.7819 - val_loss: 0.8432 - val_accuracy: 0.7167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x32b295310>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(X_train_r, y_train, epochs=20, batch_size=32, validation_data=(X_test_r, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "23/23 [==============================] - ETA: 0s - loss: 2.5997 - accuracy: 0.1542"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 02:01:46.161447: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 41s 2s/step - loss: 2.5997 - accuracy: 0.1542 - val_loss: 2.0968 - val_accuracy: 0.1889\n",
      "Epoch 2/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 2.4819 - accuracy: 0.1611 - val_loss: 1.9701 - val_accuracy: 0.1944\n",
      "Epoch 3/20\n",
      "23/23 [==============================] - 42s 2s/step - loss: 2.3203 - accuracy: 0.2056 - val_loss: 2.0602 - val_accuracy: 0.2222\n",
      "Epoch 4/20\n",
      "23/23 [==============================] - 43s 2s/step - loss: 2.3215 - accuracy: 0.1917 - val_loss: 2.0321 - val_accuracy: 0.2278\n",
      "Epoch 5/20\n",
      "23/23 [==============================] - 44s 2s/step - loss: 2.3650 - accuracy: 0.2014 - val_loss: 2.0362 - val_accuracy: 0.1889\n",
      "Epoch 6/20\n",
      "23/23 [==============================] - 45s 2s/step - loss: 2.3316 - accuracy: 0.1861 - val_loss: 2.0105 - val_accuracy: 0.1833\n",
      "Epoch 7/20\n",
      "23/23 [==============================] - 44s 2s/step - loss: 2.2731 - accuracy: 0.2306 - val_loss: 2.0468 - val_accuracy: 0.2056\n",
      "Epoch 8/20\n",
      "23/23 [==============================] - 43s 2s/step - loss: 2.2647 - accuracy: 0.2125 - val_loss: 1.9230 - val_accuracy: 0.2167\n",
      "Epoch 9/20\n",
      "23/23 [==============================] - 46s 2s/step - loss: 2.1322 - accuracy: 0.2222 - val_loss: 1.9365 - val_accuracy: 0.2778\n",
      "Epoch 10/20\n",
      "23/23 [==============================] - 54s 2s/step - loss: 2.2004 - accuracy: 0.2236 - val_loss: 2.0735 - val_accuracy: 0.1611\n",
      "Epoch 11/20\n",
      "23/23 [==============================] - 44s 2s/step - loss: 2.1112 - accuracy: 0.2694 - val_loss: 1.8904 - val_accuracy: 0.2722\n",
      "Epoch 12/20\n",
      "23/23 [==============================] - 44s 2s/step - loss: 2.0883 - accuracy: 0.2792 - val_loss: 1.7650 - val_accuracy: 0.3333\n",
      "Epoch 13/20\n",
      "23/23 [==============================] - 42s 2s/step - loss: 1.9761 - accuracy: 0.2861 - val_loss: 1.7520 - val_accuracy: 0.3611\n",
      "Epoch 14/20\n",
      "23/23 [==============================] - 42s 2s/step - loss: 1.9344 - accuracy: 0.2917 - val_loss: 1.7197 - val_accuracy: 0.3611\n",
      "Epoch 15/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 1.9198 - accuracy: 0.2875 - val_loss: 1.7780 - val_accuracy: 0.3333\n",
      "Epoch 16/20\n",
      "23/23 [==============================] - 42s 2s/step - loss: 1.8855 - accuracy: 0.3236 - val_loss: 1.7589 - val_accuracy: 0.3667\n",
      "Epoch 17/20\n",
      "23/23 [==============================] - 43s 2s/step - loss: 1.9087 - accuracy: 0.2986 - val_loss: 1.6663 - val_accuracy: 0.3444\n",
      "Epoch 18/20\n",
      "23/23 [==============================] - 43s 2s/step - loss: 1.8566 - accuracy: 0.3292 - val_loss: 1.6970 - val_accuracy: 0.3556\n",
      "Epoch 19/20\n",
      "23/23 [==============================] - 48s 2s/step - loss: 1.8891 - accuracy: 0.3264 - val_loss: 1.7166 - val_accuracy: 0.3722\n",
      "Epoch 20/20\n",
      "23/23 [==============================] - 48s 2s/step - loss: 1.9356 - accuracy: 0.2986 - val_loss: 1.6915 - val_accuracy: 0.3333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3e0868b50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model.fit(X_train_r, y_train, epochs=20, batch_size=32, validation_data=(X_test_r, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/tensorflow-test/env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Music Genre Classification Models Trained and Saved Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save models\n",
    "import joblib\n",
    "svm_model_path = \"/Users/js/Desktop/Music Genre Classification/Models/svm_model.pkl\"\n",
    "cnn_model_path = \"/Users/js/Desktop/Music Genre Classification/Models/cnn_model.h5\"\n",
    "lstm_model_path = \"/Users/js/Desktop/Music Genre Classification/Models/lstm_model.h5\"\n",
    "transformer_model_path = \"/Users/js/Desktop/Music Genre Classification/Models/transformer_model.h5\"\n",
    "\n",
    "\n",
    "joblib.dump(svm_model, svm_model_path)\n",
    "cnn_model.save(cnn_model_path)\n",
    "lstm_model.save(lstm_model_path)\n",
    "transformer_model.save(transformer_model_path)\n",
    "\n",
    "print(\"Music Genre Classification Models Trained and Saved Successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YAMNet Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For debugging purpose, can ignore the code below.\n",
    "- Contains SVM model trained on just YAMNet extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 14:32:27.696333: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAMNet feature extraction complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_path = \"/Users/js/Desktop/Music Genre Classification/Data/genres_original\"\n",
    "\n",
    "def extract_all_yamnet_features(data_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for genre in os.listdir(data_path):\n",
    "        genre_path = os.path.join(data_path, genre)\n",
    "        if os.path.isdir(genre_path):\n",
    "            for file in os.listdir(genre_path):\n",
    "                if file.endswith(\".wav\"):\n",
    "                    file_path = os.path.join(genre_path, file)\n",
    "                    features = extract_yamnet_features(file_path)\n",
    "                    if features is not None:\n",
    "                        data.append(features)\n",
    "                        labels.append(genre)\n",
    "    \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "X_yamnet, y_yamnet = extract_all_yamnet_features(data_path)\n",
    "\n",
    "df_yamnet = pd.DataFrame(X_yamnet)\n",
    "df_yamnet[\"Genre\"] = y_yamnet\n",
    "df_yamnet.to_csv(\"/Users/js/Desktop/Music Genre Classification/Data/yamnet_features.csv\", index=False)\n",
    "\n",
    "print(\"YAMNet feature extraction complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM trained on YAMNet features!\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/js/Desktop/Music Genre Classification/Data/yamnet_features.csv\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Genre\"] = label_encoder.fit_transform(df[\"Genre\"])\n",
    "\n",
    "X = df.iloc[:, :-1].values \n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C=10, gamma='scale')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"SVM trained on YAMNet features!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_audio(audio_path, model, scaler, label_encoder):\n",
    "    features = extract_yamnet_features(audio_path)\n",
    "    if features is not None:\n",
    "        features = scaler.transform([features]) \n",
    "        prediction = model.predict(features)\n",
    "        return label_encoder.inverse_transform(prediction)[0]\n",
    "    return \"Error processing audio\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hiphop'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_audio('/Users/js/Downloads/action-urban-trap-141691.wav', svm_model, scaler, label_encoder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
